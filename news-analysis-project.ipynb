{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3342665,"sourceType":"datasetVersion","datasetId":2001636},{"sourceId":8075292,"sourceType":"datasetVersion","datasetId":4765514},{"sourceId":8075647,"sourceType":"datasetVersion","datasetId":4765780},{"sourceId":8081028,"sourceType":"datasetVersion","datasetId":4769640},{"sourceId":28286,"sourceType":"modelInstanceVersion","modelInstanceId":23817}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Result using \"Assignment.xlsx\" file \n\nResult stored in \"result_with_txt.csv\" file.\n\nBest Perplexity: 1313.8285671354606\n\n","metadata":{}},{"cell_type":"markdown","source":"## Here's what perplexity signifies:\n\nIt is the evaluation method particularly used in language models like Latent Dirichlet Allocation (LDA). This score measures how well the model predicts a sample of text.\n\n1. Lower Perplexity: A lower perplexity score indicates that the model is better at predicting the sample text. In other words, the lower the perplexity, the better the model is at capturing the underlying structure of the text data.\n\n2. Higher Perplexity: Conversely, a higher perplexity score indicates poorer performance of the model. It suggests that the model has more difficulty predicting the sample text and may not be capturing the underlying patterns effectively.\n\n####  I used Perplexity instead of precision, recall, f1 score, because \"perplexity\" is the best way/method to evaluate the language based models ","metadata":{}},{"cell_type":"markdown","source":"# News Analysis Project","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport spacy\n\n# Load the dataset\ndf = pd.read_excel('/kaggle/input/dataset/Assignment.xlsx', header=None, names=['Description'])\n\n# Function to clean text\nnlp = spacy.load('en_core_web_sm')\n\nimport re\nfrom bs4 import BeautifulSoup\n\ndef clean_text(text):\n    # Remove HTML tags\n    text = BeautifulSoup(text, \"html.parser\").get_text()\n    \n    # Remove special characters and digits\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    \n    # Convert text to lowercase\n    text = text.lower()\n    \n    # Lemmatize using spaCy\n    doc = nlp(text)\n    lemmatized_tokens = [token.lemma_ for token in doc if not token.is_stop]\n    \n    # Join tokens back into a string\n    cleaned_text = ' '.join(lemmatized_tokens)\n    \n    return cleaned_text\n\ndf['Cleaned_Description'] = df['Description'].apply(clean_text)\n\n\n# Function to get sentiment\ndef get_sentiment(text):\n    sid = SentimentIntensityAnalyzer()\n    sentiment_scores = sid.polarity_scores(text)\n    if sentiment_scores['compound'] >= 0.05:\n        return 'Positive'\n    elif sentiment_scores['compound'] <= -0.05:\n        return 'Negative'\n    else:\n        return 'Neutral'\n\ndf['Sentiment'] = df['Cleaned_Description'].apply(get_sentiment)\n\n# Find topics\ndef find_topics(texts):\n    vectorizer = TfidfVectorizer(max_df=0.9, min_df=0.03, stop_words='english', max_features=150)\n    tfidf = vectorizer.fit_transform(texts)\n    \n    # Hyperparameter tuning for LDA\n    best_perplexity = float('inf')\n    best_lda_model = None\n    for n_topics in range(5, 50):  # Trying different numbers of topics\n        lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n        lda_model.fit(tfidf)\n        perplexity = lda_model.perplexity(tfidf)\n        if perplexity < best_perplexity:\n            best_perplexity = perplexity\n            best_lda_model = lda_model\n    \n    print(f'Best Perplexity: {best_perplexity}')\n    \n    # Extracting topics from the best model\n    feature_names = vectorizer.get_feature_names_out()\n    topics = []\n    for text in texts:\n        doc = nlp(text)\n        tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n        tfidf_vector = vectorizer.transform([' '.join(tokens)])\n        topic_distribution = best_lda_model.transform(tfidf_vector)\n        top_topic = topic_distribution.argmax(axis=1)[0]\n        top_features_ind = best_lda_model.components_[top_topic].argsort()[:-10 - 1:-1]\n        top_features = [feature_names[i] for i in top_features_ind]\n        topics.append(', '.join(top_features))\n    return topics\n\ndf['Topics'] = find_topics(df['Cleaned_Description'])\n\n# Function to get aspects\ndef aspect_analysis(text):\n    doc = nlp(text)\n    aspects = set()  # Using a set to avoid duplicates\n    # Additional entity types to consider\n    additional_entity_types = ['PERSON', 'GPE', 'NORP', 'FAC', 'LOC', 'LAW', 'LANGUAGE', 'DATE', 'TIME', 'QUANTITY', 'ORDINAL', 'CARDINAL']\n    for ent in doc.ents:\n        if ent.label_ in ['ORG', 'PRODUCT', 'EVENT'] or ent.label_ in additional_entity_types:\n            aspects.add((ent.text, ent.label_))\n    return list(aspects)\n\ndf['Aspects'] = df['Description'].apply(aspect_analysis)\n\n# Save the results to a new CSV file\nresult_df1 = df[['Description', 'Cleaned_Description', 'Sentiment', 'Topics', 'Aspects']]\nresult_df1.to_csv('result_with_excel.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:07:54.590848Z","iopub.execute_input":"2024-04-10T12:07:54.591828Z","iopub.status.idle":"2024-04-10T12:08:10.837581Z","shell.execute_reply.started":"2024-04-10T12:07:54.591775Z","shell.execute_reply":"2024-04-10T12:08:10.835801Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Best Perplexity: 1313.8285671354606\n","output_type":"stream"}]},{"cell_type":"code","source":"result_df1.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T12:08:10.839771Z","iopub.execute_input":"2024-04-10T12:08:10.840605Z","iopub.status.idle":"2024-04-10T12:08:10.944890Z","shell.execute_reply.started":"2024-04-10T12:08:10.840555Z","shell.execute_reply":"2024-04-10T12:08:10.943054Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"                                         Description  \\\n0                                            Article   \n1  Retailers, the makers of foods marketed for we...   \n2  Move over, Ozempic — there’s a new drug in tow...   \n3  Sept 14 (Reuters) - Bristol Myers Squibb (BMY....   \n4  Austin Wolcott was 18 years old and pretty sur...   \n\n                                 Cleaned_Description Sentiment  \\\n0                                            article   Neutral   \n1  retailer maker food market weight loss type co...  Positive   \n2  ozempic   s new drug town \\n\\n eli lillys zepb...  Negative   \n3  sept   reuters   bristol myers squibb bmyn say...  Negative   \n4  austin wolcott   year old pretty sure not surv...  Negative   \n\n                                              Topics  \\\n0  account, nike, accord, cancer, strava, use, cl...   \n1  drug, weight, like, people, approve, chief, ye...   \n2  drug, weight, like, people, approve, chief, ye...   \n3  therapy, cancer, cart, treatment, drug, cell, ...   \n4  therapy, cancer, cart, treatment, drug, cell, ...   \n\n                                             Aspects  \n0                                                 []  \n1  [(U.S., GPE), (Ozempic, PERSON), (C.E.O.s, ORG...  \n2  [(the end of the year, DATE), (Zepbound, PERSO...  \n3  [(Pfizer, ORG), (10, CARDINAL), (Eliquis, PERS...  \n4  [(John McCain, PERSON), (the ‘90s, DATE), (Mic...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Description</th>\n      <th>Cleaned_Description</th>\n      <th>Sentiment</th>\n      <th>Topics</th>\n      <th>Aspects</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Article</td>\n      <td>article</td>\n      <td>Neutral</td>\n      <td>account, nike, accord, cancer, strava, use, cl...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Retailers, the makers of foods marketed for we...</td>\n      <td>retailer maker food market weight loss type co...</td>\n      <td>Positive</td>\n      <td>drug, weight, like, people, approve, chief, ye...</td>\n      <td>[(U.S., GPE), (Ozempic, PERSON), (C.E.O.s, ORG...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Move over, Ozempic — there’s a new drug in tow...</td>\n      <td>ozempic   s new drug town \\n\\n eli lillys zepb...</td>\n      <td>Negative</td>\n      <td>drug, weight, like, people, approve, chief, ye...</td>\n      <td>[(the end of the year, DATE), (Zepbound, PERSO...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Sept 14 (Reuters) - Bristol Myers Squibb (BMY....</td>\n      <td>sept   reuters   bristol myers squibb bmyn say...</td>\n      <td>Negative</td>\n      <td>therapy, cancer, cart, treatment, drug, cell, ...</td>\n      <td>[(Pfizer, ORG), (10, CARDINAL), (Eliquis, PERS...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Austin Wolcott was 18 years old and pretty sur...</td>\n      <td>austin wolcott   year old pretty sure not surv...</td>\n      <td>Negative</td>\n      <td>therapy, cancer, cart, treatment, drug, cell, ...</td>\n      <td>[(John McCain, PERSON), (the ‘90s, DATE), (Mic...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Result using cnn Articles \n### result stored in \"result_with_sentiment_and_aspects.csv\" file \n\n#### Best Perplexity score : 189.35439836654564 \n\n\n","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# from sklearn.decomposition import LatentDirichletAllocation\n# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n# import spacy\n\n# # Load the dataset\n# df = pd.read_csv('/kaggle/input/cnn-articles-after-basic-cleaning/CNN_Articels_clean/CNN_Articels_clean.csv')\n\n# # Function to clean text\n# nlp = spacy.load('en_core_web_sm')\n\n# def clean_text(text):\n#     doc = nlp(text)\n#     lemmatized_tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n#     cleaned_text = ' '.join(lemmatized_tokens)\n#     return cleaned_text\n\n# df['Cleaned_Description'] = df['Description'].apply(clean_text)\n\n# # Function to get sentiment\n# def get_sentiment(text):\n#     sid = SentimentIntensityAnalyzer()\n#     sentiment_scores = sid.polarity_scores(text)\n#     if sentiment_scores['compound'] >= 0.05:\n#         return 'Positive'\n#     elif sentiment_scores['compound'] <= -0.05:\n#         return 'Negative'\n#     else:\n#         return 'Neutral'\n\n# df['Sentiment'] = df['Cleaned_Description'].apply(get_sentiment)\n\n# # Find topics\n# def find_topics(texts):\n#     vectorizer = TfidfVectorizer(max_df=0.8, min_df=0.02, stop_words='english', max_features=3500)\n#     tfidf = vectorizer.fit_transform(texts)\n    \n#     # Hyperparameter tuning for LDA\n#     best_perplexity = float('inf')\n#     best_lda_model = None\n#     for n_topics in range(5, 51):  # Trying different numbers of topics\n#         lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n#         lda_model.fit(tfidf)\n#         perplexity = lda_model.perplexity(tfidf)\n#         if perplexity < best_perplexity:\n#             best_perplexity = perplexity\n#             best_lda_model = lda_model\n    \n#     print(f'Best Perplexity: {best_perplexity}')\n    \n#     # Extracting topics from the best model\n#     feature_names = vectorizer.get_feature_names_out()\n#     topics = []\n#     for text in texts:\n#         doc = nlp(text)\n#         tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n#         tfidf_vector = vectorizer.transform([' '.join(tokens)])\n#         topic_distribution = best_lda_model.transform(tfidf_vector)\n#         top_topic = topic_distribution.argmax(axis=1)[0]\n#         top_features_ind = best_lda_model.components_[top_topic].argsort()[:-10 - 1:-1]\n#         top_features = [feature_names[i] for i in top_features_ind]\n#         topics.append(', '.join(top_features))\n#     return topics\n\n# df['Topics'] = find_topics(df['Cleaned_Description'])\n\n# # Function to get aspects\n# def aspect_analysis(text):\n#     doc = nlp(text)\n#     aspects = set()  # Using a set to avoid duplicates\n#     # Additional entity types to consider\n#     additional_entity_types = ['PERSON', 'GPE', 'NORP', 'FAC', 'LOC', 'LAW', 'LANGUAGE', 'DATE', 'TIME', 'QUANTITY', 'ORDINAL', 'CARDINAL']\n#     for ent in doc.ents:\n#         if ent.label_ in ['ORG', 'PRODUCT', 'EVENT'] or ent.label_ in additional_entity_types:\n#             aspects.add((ent.text, ent.label_))\n#     return list(aspects)\n\n# df['Aspects'] = df['Description'].apply(aspect_analysis)\n\n# # Save the results to a new CSV file\n# result_df = df[['Description', 'Cleaned_Description', 'Sentiment', 'Topics', 'Aspects']]\n# result_df.to_csv('result_with_sentiment_and_aspects.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:47:46.213243Z","iopub.execute_input":"2024-04-10T09:47:46.214242Z","iopub.status.idle":"2024-04-10T09:53:34.368248Z","shell.execute_reply.started":"2024-04-10T09:47:46.214146Z","shell.execute_reply":"2024-04-10T09:53:34.365618Z"},"trusted":true},"execution_count":9,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m         topics\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(top_features))\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m topics\n\u001b[0;32m---> 66\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTopics\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfind_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCleaned_Description\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Function to get aspects\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maspect_analysis\u001b[39m(text):\n","Cell \u001b[0;32mIn[9], line 44\u001b[0m, in \u001b[0;36mfind_topics\u001b[0;34m(texts)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_topics \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m51\u001b[39m):  \u001b[38;5;66;03m# Trying different numbers of topics\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     lda_model \u001b[38;5;241m=\u001b[39m LatentDirichletAllocation(n_components\u001b[38;5;241m=\u001b[39mn_topics, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mlda_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfidf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     perplexity \u001b[38;5;241m=\u001b[39m lda_model\u001b[38;5;241m.\u001b[39mperplexity(tfidf)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m perplexity \u001b[38;5;241m<\u001b[39m best_perplexity:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/decomposition/_lda.py:668\u001b[0m, in \u001b[0;36mLatentDirichletAllocation.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_em_step(\n\u001b[1;32m    661\u001b[0m             X[idx_slice, :],\n\u001b[1;32m    662\u001b[0m             total_samples\u001b[38;5;241m=\u001b[39mn_samples,\n\u001b[1;32m    663\u001b[0m             batch_update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    664\u001b[0m             parallel\u001b[38;5;241m=\u001b[39mparallel,\n\u001b[1;32m    665\u001b[0m         )\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;66;03m# batch update\u001b[39;00m\n\u001b[0;32m--> 668\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_em_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_update\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;66;03m# check perplexity\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_every \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m evaluate_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/decomposition/_lda.py:517\u001b[0m, in \u001b[0;36mLatentDirichletAllocation._em_step\u001b[0;34m(self, X, total_samples, batch_update, parallel)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"EM update for 1 iteration.\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \n\u001b[1;32m    492\u001b[0m \u001b[38;5;124;03mupdate `_component` by batch VB or online VB.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;124;03m    Unnormalized document topic distribution.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;66;03m# E-step\u001b[39;00m\n\u001b[0;32m--> 517\u001b[0m _, suff_stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcal_sstats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# M-step\u001b[39;00m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_update:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/decomposition/_lda.py:460\u001b[0m, in \u001b[0;36mLatentDirichletAllocation._e_step\u001b[0;34m(self, X, cal_sstats, random_init, parallel)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parallel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 460\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_update_doc_distribution\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_slice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp_dirichlet_component_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdoc_topic_prior_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_doc_update_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean_change_tol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcal_sstats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx_slice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen_even_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;66;03m# merge result\u001b[39;00m\n\u001b[1;32m    474\u001b[0m doc_topics, sstats_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/decomposition/_lda.py:132\u001b[0m, in \u001b[0;36m_update_doc_distribution\u001b[0;34m(X, exp_topic_word_distr, doc_topic_prior, max_doc_update_iter, mean_change_tol, cal_sstats, random_state)\u001b[0m\n\u001b[1;32m    129\u001b[0m exp_topic_word_d \u001b[38;5;241m=\u001b[39m exp_topic_word_distr[:, ids]\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Iterate between `doc_topic_d` and `norm_phi` until convergence\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_doc_update_iter\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    133\u001b[0m     last_d \u001b[38;5;241m=\u001b[39m doc_topic_d\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# The optimal phi_{dwk} is proportional to\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# exp(E[log(theta_{dk})]) * exp(E[log(beta_{dw})]).\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# result_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T19:36:47.102951Z","iopub.execute_input":"2024-04-09T19:36:47.103793Z","iopub.status.idle":"2024-04-09T19:36:47.177101Z","shell.execute_reply.started":"2024-04-09T19:36:47.103748Z","shell.execute_reply":"2024-04-09T19:36:47.175976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:58:53.338492Z","iopub.execute_input":"2024-04-10T09:58:53.338972Z","iopub.status.idle":"2024-04-10T09:58:53.435785Z","shell.execute_reply.started":"2024-04-10T09:58:53.338936Z","shell.execute_reply":"2024-04-10T09:58:53.433847Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                                         Description  \\\n0                                            Article   \n1  Retailers, the makers of foods marketed for we...   \n2  Move over, Ozempic — there’s a new drug in tow...   \n3  Sept 14 (Reuters) - Bristol Myers Squibb (BMY....   \n4  Austin Wolcott was 18 years old and pretty sur...   \n\n                                 Cleaned_Description Sentiment  \\\n0                                            article   Neutral   \n1  retailer maker food market weight loss type co...  Positive   \n2  ozempic   s new drug town \\n\\n eli lillys zepb...  Negative   \n3  sept   reuters   bristol myers squibb bmyn say...  Negative   \n4  austin wolcott   year old pretty sure not surv...  Negative   \n\n                                              Topics  \\\n0  account, nike, accord, cancer, strava, use, cl...   \n1  drug, weight, like, people, approve, chief, ye...   \n2  drug, weight, like, people, approve, chief, ye...   \n3  therapy, cancer, cart, treatment, drug, cell, ...   \n4  therapy, cancer, cart, treatment, drug, cell, ...   \n\n                                             Aspects  \n0                                                 []  \n1  [(U.S., GPE), (Ozempic, PERSON), (C.E.O.s, ORG...  \n2  [(the end of the year, DATE), (Zepbound, PERSO...  \n3  [(Pfizer, ORG), (10, CARDINAL), (Eliquis, PERS...  \n4  [(John McCain, PERSON), (the ‘90s, DATE), (Mic...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Description</th>\n      <th>Cleaned_Description</th>\n      <th>Sentiment</th>\n      <th>Topics</th>\n      <th>Aspects</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Article</td>\n      <td>article</td>\n      <td>Neutral</td>\n      <td>account, nike, accord, cancer, strava, use, cl...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Retailers, the makers of foods marketed for we...</td>\n      <td>retailer maker food market weight loss type co...</td>\n      <td>Positive</td>\n      <td>drug, weight, like, people, approve, chief, ye...</td>\n      <td>[(U.S., GPE), (Ozempic, PERSON), (C.E.O.s, ORG...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Move over, Ozempic — there’s a new drug in tow...</td>\n      <td>ozempic   s new drug town \\n\\n eli lillys zepb...</td>\n      <td>Negative</td>\n      <td>drug, weight, like, people, approve, chief, ye...</td>\n      <td>[(the end of the year, DATE), (Zepbound, PERSO...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Sept 14 (Reuters) - Bristol Myers Squibb (BMY....</td>\n      <td>sept   reuters   bristol myers squibb bmyn say...</td>\n      <td>Negative</td>\n      <td>therapy, cancer, cart, treatment, drug, cell, ...</td>\n      <td>[(Pfizer, ORG), (10, CARDINAL), (Eliquis, PERS...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Austin Wolcott was 18 years old and pretty sur...</td>\n      <td>austin wolcott   year old pretty sure not surv...</td>\n      <td>Negative</td>\n      <td>therapy, cancer, cart, treatment, drug, cell, ...</td>\n      <td>[(John McCain, PERSON), (the ‘90s, DATE), (Mic...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Future Considerations\n\n1. We can feed more data to the model. \n\n2. We can integration with Advanced NLP Models to get more better results .\n\n3. Dynamic Topic Modeling: Develop dynamic topic modeling techniques to capture the evolving nature of topics in large text corpora over time, enabling more accurate and up-to-date topic analysis.\n\n4. we can create interactive visualization tools to allow users to explore topics, sentiment, and other insights in text data interactively, facilitating deeper understanding and analysis.\n\n5. Incorporate multimodal analysis techniques to analyze both textual and visual data together, enabling richer insights from diverse data sources such as images, videos, and text.\n\n6. Implement real-time topic detection algorithms to identify emerging topics and trends as they happen, enabling timely decision-making and response in various applications such as social media monitoring and news analysis.\n\n7. Cross-domain Analysis\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}