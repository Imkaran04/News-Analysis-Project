{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be028e33",
   "metadata": {
    "papermill": {
     "duration": 0.006212,
     "end_time": "2024-04-10T12:09:22.357843",
     "exception": false,
     "start_time": "2024-04-10T12:09:22.351631",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Result using \"Assignment.xlsx\" file \n",
    "\n",
    "Result stored in \"result_with_txt.csv\" file.\n",
    "\n",
    "Best Perplexity: 1313.8285671354606\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0503853a",
   "metadata": {
    "papermill": {
     "duration": 0.005144,
     "end_time": "2024-04-10T12:09:22.369261",
     "exception": false,
     "start_time": "2024-04-10T12:09:22.364117",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Here's what perplexity signifies:\n",
    "\n",
    "It is the evaluation method particularly used in language models like Latent Dirichlet Allocation (LDA). This score measures how well the model predicts a sample of text.\n",
    "\n",
    "1. Lower Perplexity: A lower perplexity score indicates that the model is better at predicting the sample text. In other words, the lower the perplexity, the better the model is at capturing the underlying structure of the text data.\n",
    "\n",
    "2. Higher Perplexity: Conversely, a higher perplexity score indicates poorer performance of the model. It suggests that the model has more difficulty predicting the sample text and may not be capturing the underlying patterns effectively.\n",
    "\n",
    "####  I used Perplexity instead of precision, recall, f1 score, because \"perplexity\" is the best way/method to evaluate the language based models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb55ac7f",
   "metadata": {
    "papermill": {
     "duration": 0.004982,
     "end_time": "2024-04-10T12:09:22.379581",
     "exception": false,
     "start_time": "2024-04-10T12:09:22.374599",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# News Analysis Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a958d3be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T12:09:22.393444Z",
     "iopub.status.busy": "2024-04-10T12:09:22.392166Z",
     "iopub.status.idle": "2024-04-10T12:09:48.744494Z",
     "shell.execute_reply": "2024-04-10T12:09:48.743365Z"
    },
    "papermill": {
     "duration": 26.362959,
     "end_time": "2024-04-10T12:09:48.747840",
     "exception": false,
     "start_time": "2024-04-10T12:09:22.384881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Perplexity: 1313.8285671354606\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_excel('/kaggle/input/dataset/Assignment.xlsx', header=None, names=['Description'])\n",
    "\n",
    "# Function to clean text\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Lemmatize using spaCy\n",
    "    doc = nlp(text)\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    \n",
    "    # Join tokens back into a string\n",
    "    cleaned_text = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "df['Cleaned_Description'] = df['Description'].apply(clean_text)\n",
    "\n",
    "\n",
    "# Function to get sentiment\n",
    "def get_sentiment(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    if sentiment_scores['compound'] >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif sentiment_scores['compound'] <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "df['Sentiment'] = df['Cleaned_Description'].apply(get_sentiment)\n",
    "\n",
    "# Find topics\n",
    "def find_topics(texts):\n",
    "    vectorizer = TfidfVectorizer(max_df=0.9, min_df=0.03, stop_words='english', max_features=150)\n",
    "    tfidf = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    # Hyperparameter tuning for LDA\n",
    "    best_perplexity = float('inf')\n",
    "    best_lda_model = None\n",
    "    for n_topics in range(5, 50):  # Trying different numbers of topics\n",
    "        lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "        lda_model.fit(tfidf)\n",
    "        perplexity = lda_model.perplexity(tfidf)\n",
    "        if perplexity < best_perplexity:\n",
    "            best_perplexity = perplexity\n",
    "            best_lda_model = lda_model\n",
    "    \n",
    "    print(f'Best Perplexity: {best_perplexity}')\n",
    "    \n",
    "    # Extracting topics from the best model\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topics = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "        tfidf_vector = vectorizer.transform([' '.join(tokens)])\n",
    "        topic_distribution = best_lda_model.transform(tfidf_vector)\n",
    "        top_topic = topic_distribution.argmax(axis=1)[0]\n",
    "        top_features_ind = best_lda_model.components_[top_topic].argsort()[:-10 - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        topics.append(', '.join(top_features))\n",
    "    return topics\n",
    "\n",
    "df['Topics'] = find_topics(df['Cleaned_Description'])\n",
    "\n",
    "# Function to get aspects\n",
    "def aspect_analysis(text):\n",
    "    doc = nlp(text)\n",
    "    aspects = set()  # Using a set to avoid duplicates\n",
    "    # Additional entity types to consider\n",
    "    additional_entity_types = ['PERSON', 'GPE', 'NORP', 'FAC', 'LOC', 'LAW', 'LANGUAGE', 'DATE', 'TIME', 'QUANTITY', 'ORDINAL', 'CARDINAL']\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['ORG', 'PRODUCT', 'EVENT'] or ent.label_ in additional_entity_types:\n",
    "            aspects.add((ent.text, ent.label_))\n",
    "    return list(aspects)\n",
    "\n",
    "df['Aspects'] = df['Description'].apply(aspect_analysis)\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "result_df1 = df[['Description', 'Cleaned_Description', 'Sentiment', 'Topics', 'Aspects']]\n",
    "result_df1.to_csv('result_with_excel.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e34e430",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T12:09:48.763520Z",
     "iopub.status.busy": "2024-04-10T12:09:48.762982Z",
     "iopub.status.idle": "2024-04-10T12:09:48.863883Z",
     "shell.execute_reply": "2024-04-10T12:09:48.862574Z"
    },
    "papermill": {
     "duration": 0.111728,
     "end_time": "2024-04-10T12:09:48.866749",
     "exception": false,
     "start_time": "2024-04-10T12:09:48.755021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Cleaned_Description</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Topics</th>\n",
       "      <th>Aspects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Article</td>\n",
       "      <td>article</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>account, nike, accord, cancer, strava, use, cl...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Retailers, the makers of foods marketed for we...</td>\n",
       "      <td>retailer maker food market weight loss type co...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>drug, weight, like, people, approve, chief, ye...</td>\n",
       "      <td>[(hours, TIME), (obese, NORP), (American, NORP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Move over, Ozempic — there’s a new drug in tow...</td>\n",
       "      <td>ozempic   s new drug town \\n\\n eli lillys zepb...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>drug, weight, like, people, approve, chief, ye...</td>\n",
       "      <td>[(obese, NORP), (Tirzepatide, GPE), (the Divis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sept 14 (Reuters) - Bristol Myers Squibb (BMY....</td>\n",
       "      <td>sept   reuters   bristol myers squibb bmyn say...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>therapy, cancer, cart, treatment, drug, cell, ...</td>\n",
       "      <td>[(Revlimid, PERSON), (Eliquis, PERSON), (Abecm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Austin Wolcott was 18 years old and pretty sur...</td>\n",
       "      <td>austin wolcott   year old pretty sure not surv...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>therapy, cancer, cart, treatment, drug, cell, ...</td>\n",
       "      <td>[(North Carolina, GPE), (just a few minutes, T...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Description  \\\n",
       "0                                            Article   \n",
       "1  Retailers, the makers of foods marketed for we...   \n",
       "2  Move over, Ozempic — there’s a new drug in tow...   \n",
       "3  Sept 14 (Reuters) - Bristol Myers Squibb (BMY....   \n",
       "4  Austin Wolcott was 18 years old and pretty sur...   \n",
       "\n",
       "                                 Cleaned_Description Sentiment  \\\n",
       "0                                            article   Neutral   \n",
       "1  retailer maker food market weight loss type co...  Positive   \n",
       "2  ozempic   s new drug town \\n\\n eli lillys zepb...  Negative   \n",
       "3  sept   reuters   bristol myers squibb bmyn say...  Negative   \n",
       "4  austin wolcott   year old pretty sure not surv...  Negative   \n",
       "\n",
       "                                              Topics  \\\n",
       "0  account, nike, accord, cancer, strava, use, cl...   \n",
       "1  drug, weight, like, people, approve, chief, ye...   \n",
       "2  drug, weight, like, people, approve, chief, ye...   \n",
       "3  therapy, cancer, cart, treatment, drug, cell, ...   \n",
       "4  therapy, cancer, cart, treatment, drug, cell, ...   \n",
       "\n",
       "                                             Aspects  \n",
       "0                                                 []  \n",
       "1  [(hours, TIME), (obese, NORP), (American, NORP...  \n",
       "2  [(obese, NORP), (Tirzepatide, GPE), (the Divis...  \n",
       "3  [(Revlimid, PERSON), (Eliquis, PERSON), (Abecm...  \n",
       "4  [(North Carolina, GPE), (just a few minutes, T...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083b8cc1",
   "metadata": {
    "papermill": {
     "duration": 0.006377,
     "end_time": "2024-04-10T12:09:48.879777",
     "exception": false,
     "start_time": "2024-04-10T12:09:48.873400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861fdfde",
   "metadata": {
    "papermill": {
     "duration": 0.00606,
     "end_time": "2024-04-10T12:09:48.892375",
     "exception": false,
     "start_time": "2024-04-10T12:09:48.886315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf21594",
   "metadata": {
    "papermill": {
     "duration": 0.006062,
     "end_time": "2024-04-10T12:09:48.904845",
     "exception": false,
     "start_time": "2024-04-10T12:09:48.898783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4010e4",
   "metadata": {
    "papermill": {
     "duration": 0.005967,
     "end_time": "2024-04-10T12:09:48.917211",
     "exception": false,
     "start_time": "2024-04-10T12:09:48.911244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37e1dcac",
   "metadata": {
    "papermill": {
     "duration": 0.005915,
     "end_time": "2024-04-10T12:09:48.929468",
     "exception": false,
     "start_time": "2024-04-10T12:09:48.923553",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Result using cnn Articles \n",
    "### result stored in \"result_with_sentiment_and_aspects.csv\" file \n",
    "\n",
    "#### Best Perplexity score : 189.35439836654564 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba582434",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T12:09:48.947974Z",
     "iopub.status.busy": "2024-04-10T12:09:48.946911Z",
     "iopub.status.idle": "2024-04-10T12:09:48.955149Z",
     "shell.execute_reply": "2024-04-10T12:09:48.954286Z"
    },
    "papermill": {
     "duration": 0.020472,
     "end_time": "2024-04-10T12:09:48.957850",
     "exception": false,
     "start_time": "2024-04-10T12:09:48.937378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# import spacy\n",
    "\n",
    "# # Load the dataset\n",
    "# df = pd.read_csv('/kaggle/input/cnn-articles-after-basic-cleaning/CNN_Articels_clean/CNN_Articels_clean.csv')\n",
    "\n",
    "# # Function to clean text\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# def clean_text(text):\n",
    "#     doc = nlp(text)\n",
    "#     lemmatized_tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "#     cleaned_text = ' '.join(lemmatized_tokens)\n",
    "#     return cleaned_text\n",
    "\n",
    "# df['Cleaned_Description'] = df['Description'].apply(clean_text)\n",
    "\n",
    "# # Function to get sentiment\n",
    "# def get_sentiment(text):\n",
    "#     sid = SentimentIntensityAnalyzer()\n",
    "#     sentiment_scores = sid.polarity_scores(text)\n",
    "#     if sentiment_scores['compound'] >= 0.05:\n",
    "#         return 'Positive'\n",
    "#     elif sentiment_scores['compound'] <= -0.05:\n",
    "#         return 'Negative'\n",
    "#     else:\n",
    "#         return 'Neutral'\n",
    "\n",
    "# df['Sentiment'] = df['Cleaned_Description'].apply(get_sentiment)\n",
    "\n",
    "# # Find topics\n",
    "# def find_topics(texts):\n",
    "#     vectorizer = TfidfVectorizer(max_df=0.8, min_df=0.02, stop_words='english', max_features=3500)\n",
    "#     tfidf = vectorizer.fit_transform(texts)\n",
    "    \n",
    "#     # Hyperparameter tuning for LDA\n",
    "#     best_perplexity = float('inf')\n",
    "#     best_lda_model = None\n",
    "#     for n_topics in range(5, 51):  # Trying different numbers of topics\n",
    "#         lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "#         lda_model.fit(tfidf)\n",
    "#         perplexity = lda_model.perplexity(tfidf)\n",
    "#         if perplexity < best_perplexity:\n",
    "#             best_perplexity = perplexity\n",
    "#             best_lda_model = lda_model\n",
    "    \n",
    "#     print(f'Best Perplexity: {best_perplexity}')\n",
    "    \n",
    "#     # Extracting topics from the best model\n",
    "#     feature_names = vectorizer.get_feature_names_out()\n",
    "#     topics = []\n",
    "#     for text in texts:\n",
    "#         doc = nlp(text)\n",
    "#         tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "#         tfidf_vector = vectorizer.transform([' '.join(tokens)])\n",
    "#         topic_distribution = best_lda_model.transform(tfidf_vector)\n",
    "#         top_topic = topic_distribution.argmax(axis=1)[0]\n",
    "#         top_features_ind = best_lda_model.components_[top_topic].argsort()[:-10 - 1:-1]\n",
    "#         top_features = [feature_names[i] for i in top_features_ind]\n",
    "#         topics.append(', '.join(top_features))\n",
    "#     return topics\n",
    "\n",
    "# df['Topics'] = find_topics(df['Cleaned_Description'])\n",
    "\n",
    "# # Function to get aspects\n",
    "# def aspect_analysis(text):\n",
    "#     doc = nlp(text)\n",
    "#     aspects = set()  # Using a set to avoid duplicates\n",
    "#     # Additional entity types to consider\n",
    "#     additional_entity_types = ['PERSON', 'GPE', 'NORP', 'FAC', 'LOC', 'LAW', 'LANGUAGE', 'DATE', 'TIME', 'QUANTITY', 'ORDINAL', 'CARDINAL']\n",
    "#     for ent in doc.ents:\n",
    "#         if ent.label_ in ['ORG', 'PRODUCT', 'EVENT'] or ent.label_ in additional_entity_types:\n",
    "#             aspects.add((ent.text, ent.label_))\n",
    "#     return list(aspects)\n",
    "\n",
    "# df['Aspects'] = df['Description'].apply(aspect_analysis)\n",
    "\n",
    "# # Save the results to a new CSV file\n",
    "# result_df = df[['Description', 'Cleaned_Description', 'Sentiment', 'Topics', 'Aspects']]\n",
    "# result_df.to_csv('result_with_sentiment_and_aspects.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "524ddaac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T12:09:48.973566Z",
     "iopub.status.busy": "2024-04-10T12:09:48.972716Z",
     "iopub.status.idle": "2024-04-10T12:09:48.977735Z",
     "shell.execute_reply": "2024-04-10T12:09:48.976590Z"
    },
    "papermill": {
     "duration": 0.016288,
     "end_time": "2024-04-10T12:09:48.980711",
     "exception": false,
     "start_time": "2024-04-10T12:09:48.964423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# result_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb01431",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T09:58:53.338972Z",
     "iopub.status.busy": "2024-04-10T09:58:53.338492Z",
     "iopub.status.idle": "2024-04-10T09:58:53.435785Z",
     "shell.execute_reply": "2024-04-10T09:58:53.433847Z",
     "shell.execute_reply.started": "2024-04-10T09:58:53.338936Z"
    },
    "papermill": {
     "duration": 0.006024,
     "end_time": "2024-04-10T12:09:48.993156",
     "exception": false,
     "start_time": "2024-04-10T12:09:48.987132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "866d72c0",
   "metadata": {
    "papermill": {
     "duration": 0.006391,
     "end_time": "2024-04-10T12:09:49.005924",
     "exception": false,
     "start_time": "2024-04-10T12:09:48.999533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Future Considerations\n",
    "\n",
    "1. We can feed more data to the model. \n",
    "\n",
    "2. We can integration with Advanced NLP Models to get more better results .\n",
    "\n",
    "3. Dynamic Topic Modeling: Develop dynamic topic modeling techniques to capture the evolving nature of topics in large text corpora over time, enabling more accurate and up-to-date topic analysis.\n",
    "\n",
    "4. we can create interactive visualization tools to allow users to explore topics, sentiment, and other insights in text data interactively, facilitating deeper understanding and analysis.\n",
    "\n",
    "5. Incorporate multimodal analysis techniques to analyze both textual and visual data together, enabling richer insights from diverse data sources such as images, videos, and text.\n",
    "\n",
    "6. Implement real-time topic detection algorithms to identify emerging topics and trends as they happen, enabling timely decision-making and response in various applications such as social media monitoring and news analysis.\n",
    "\n",
    "7. Cross-domain Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa6f3b5",
   "metadata": {
    "papermill": {
     "duration": 0.006046,
     "end_time": "2024-04-10T12:09:49.018482",
     "exception": false,
     "start_time": "2024-04-10T12:09:49.012436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2001636,
     "sourceId": 3342665,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4765514,
     "sourceId": 8075292,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4765780,
     "sourceId": 8075647,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4769640,
     "sourceId": 8081028,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 23817,
     "sourceId": 28286,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30684,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 33.366267,
   "end_time": "2024-04-10T12:09:52.100691",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-10T12:09:18.734424",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
